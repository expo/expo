{"version":3,"file":"FaceDetector.js","sourceRoot":"","sources":["../src/FaceDetector.ts"],"names":[],"mappings":"AAAA,OAAO,EAAE,mBAAmB,EAAE,MAAM,mBAAmB,CAAC;AAExD,OAAO,gBAAgB,MAAM,oBAAoB,CAAC;AAiHlD,eAAe;AACf,MAAM,CAAN,IAAY,gBAGX;AAHD,WAAY,gBAAgB;IAC1B,uDAAQ,CAAA;IACR,+DAAY,CAAA;AACd,CAAC,EAHW,gBAAgB,KAAhB,gBAAgB,QAG3B;AAED,eAAe;AACf,MAAM,CAAN,IAAY,qBAGX;AAHD,WAAY,qBAAqB;IAC/B,iEAAQ,CAAA;IACR,+DAAO,CAAA;AACT,CAAC,EAHW,qBAAqB,KAArB,qBAAqB,QAGhC;AAED,eAAe;AACf,MAAM,CAAN,IAAY,2BAGX;AAHD,WAAY,2BAA2B;IACrC,6EAAQ,CAAA;IACR,2EAAO,CAAA;AACT,CAAC,EAHW,2BAA2B,KAA3B,2BAA2B,QAGtC;AAiED,cAAc;AACd;;;;;GAKG;AACH,MAAM,CAAC,KAAK,UAAU,gBAAgB,CACpC,GAAW,EACX,UAA4B,EAAE;IAE9B,IAAI,CAAC,gBAAgB,IAAI,CAAC,gBAAgB,CAAC,WAAW,EAAE;QACtD,IAAI,MAAM,CAAC,IAAI,EAAE,OAAO,EAAE,iBAAiB,EAAE,YAAY,KAAK,MAAM,EAAE;YACpE,OAAO,CAAC,IAAI,CACV;gBACE,2KAA2K;gBAC3K,oDAAoD;gBACpD,uGAAuG;gBACvG,qEAAqE;aACtE,CAAC,IAAI,CAAC,MAAM,CAAC,CACf,CAAC;SACH;QACD,MAAM,IAAI,mBAAmB,CAAC,oBAAoB,EAAE,aAAa,CAAC,CAAC;KACpE;IACD,OAAO,MAAM,gBAAgB,CAAC,WAAW,CAAC,EAAE,GAAG,OAAO,EAAE,GAAG,EAAE,CAAC,CAAC;AACjE,CAAC","sourcesContent":["import { UnavailabilityError } from 'expo-modules-core';\n\nimport ExpoFaceDetector from './ExpoFaceDetector';\n\ndeclare const global: any;\n\n// @docsMissing\nexport type Point = { x: number; y: number };\n\n// @needsAudit\nexport type FaceFeature = {\n  /**\n   * An object containing face bounds.\n   */\n  bounds: FaceFeatureBounds;\n  /**\n   * Probability that the face is smiling. Returned only if detection classifications property is\n   * set to `FaceDetectorClassifications.all`.\n   */\n  smilingProbability?: number;\n  /**\n   * Position of the left ear in image coordinates. Returned only if detection classifications\n   * property is set to `FaceDetectorLandmarks.all`.\n   */\n  leftEarPosition?: Point;\n  /**\n   * Position of the right ear in image coordinates. Returned only if detection classifications\n   * property is set to `FaceDetectorLandmarks.all`.\n   */\n  rightEarPosition?: Point;\n  /**\n   * Position of the left eye in image coordinates. Returned only if detection classifications\n   * property is set to `FaceDetectorLandmarks.all`.\n   */\n  leftEyePosition?: Point;\n  /**\n   * Probability that the left eye is open. Returned only if detection classifications property is\n   * set to `FaceDetectorClassifications.all`.\n   */\n  leftEyeOpenProbability?: number;\n  /**\n   * Position of the right eye in image coordinates. Returned only if detection classifications\n   * property is set to `FaceDetectorLandmarks.all`.\n   */\n  rightEyePosition?: Point;\n  /**\n   * Probability that the right eye is open. Returned only if detection classifications property is\n   * set to `FaceDetectorClassifications.all`.\n   */\n  rightEyeOpenProbability?: number;\n  /**\n   * Position of the left cheek in image coordinates. Returned only if detection classifications\n   * property is set to `FaceDetectorLandmarks.all`.\n   */\n  leftCheekPosition?: Point;\n  /**\n   * Position of the right cheek in image coordinates. Returned only if detection classifications\n   * property is set to `FaceDetectorLandmarks.all`.\n   */\n  rightCheekPosition?: Point;\n  /**\n   * Position of the left edge of the mouth in image coordinates. Returned only if detection\n   * classifications property is set to `FaceDetectorLandmarks.all`.\n   */\n  leftMouthPosition?: Point;\n  /**\n   * Position of the center of the mouth in image coordinates. Returned only if detection\n   * classifications property is set to `FaceDetectorLandmarks.all`.\n   */\n  mouthPosition?: Point;\n  /**\n   * Position of the right edge of the mouth in image coordinates. Returned only if detection\n   * classifications property is set to `FaceDetectorLandmarks.all`.\n   */\n  rightMouthPosition?: Point;\n  /**\n   * Position of the bottom edge of the mouth in image coordinates. Returned only if detection\n   * classifications property is set to `FaceDetectorLandmarks.all`.\n   */\n  bottomMouthPosition?: Point;\n  /**\n   * Position of the nose base in image coordinates. Returned only if detection classifications\n   * property is set to `FaceDetectorLandmarks.all`.\n   */\n  noseBasePosition?: Point;\n  /**\n   * Yaw angle of the face (heading, turning head left or right).\n   */\n  yawAngle?: number;\n  /**\n   * Roll angle of the face (bank).\n   */\n  rollAngle?: number;\n  /**\n   * A face identifier (used for tracking, if the same face appears on consecutive frames it will\n   * have the same `faceID`).\n   */\n  faceID?: number;\n};\n\n// @needsAudit\nexport type FaceFeatureBounds = {\n  /**\n   * Size of the square containing the face in image coordinates,\n   */\n  size: {\n    width: number;\n    height: number;\n  };\n  /**\n   * Position of the top left corner of a square containing the face in image coordinates,\n   */\n  origin: Point;\n};\n\n// @docsMissing\nexport enum FaceDetectorMode {\n  fast = 1,\n  accurate = 2,\n}\n\n// @docsMissing\nexport enum FaceDetectorLandmarks {\n  none = 1,\n  all = 2,\n}\n\n// @docsMissing\nexport enum FaceDetectorClassifications {\n  none = 1,\n  all = 2,\n}\n\n// @needsAudit\nexport type Image = {\n  /**\n   * URI of the image.\n   */\n  uri: string;\n  /**\n   * Width of the image in pixels.\n   */\n  width: number;\n  /**\n   * Height of the image in pixels.\n   */\n  height: number;\n  /**\n   * Orientation of the image (value conforms to the EXIF orientation tag standard).\n   */\n  orientation: number;\n};\n\n// @needsAudit\n/**\n * In order to configure detector's behavior modules pass a settings object which is then\n * interpreted by this module.\n */\nexport type DetectionOptions = {\n  /**\n   * Whether to detect faces in fast or accurate mode. Use `FaceDetector.FaceDetectorMode.{fast, accurate}`.\n   */\n  mode?: FaceDetectorMode;\n  /**\n   * Whether to detect and return landmarks positions on the face (ears, eyes, mouth, cheeks, nose).\n   * Use `FaceDetector.FaceDetectorLandmarks.{all, none}`.\n   */\n  detectLandmarks?: FaceDetectorLandmarks;\n  /**\n   * Whether to run additional classifications on detected faces (smiling probability, open eye\n   * probabilities). Use `FaceDetector.FaceDetectorClassifications.{all, none}`.\n   */\n  runClassifications?: FaceDetectorClassifications;\n  /**\n   * Minimal interval in milliseconds between two face detection events being submitted to JS.\n   * Use, when you expect lots of faces for long time and are afraid of JS Bridge being overloaded.\n   * @default 0\n   */\n  minDetectionInterval?: number;\n  /**\n   * Flag to enable tracking of faces between frames. If true, each face will be returned with\n   * `faceID` attribute which should be consistent across frames.\n   * @default false\n   */\n  tracking?: boolean;\n};\n\n// @needsAudit\nexport type DetectionResult = {\n  /**\n   * Array of faces objects.\n   */\n  faces: FaceFeature[];\n  // @docsMissing\n  image: Image;\n};\n// @needsAudit\n/**\n * Detect faces on a picture.\n * @param uri `file://` URI to the image.\n * @param options A map of detection options.\n * @return Returns a Promise which fulfils with [`DetectionResult`](#detectionresult) object.\n */\nexport async function detectFacesAsync(\n  uri: string,\n  options: DetectionOptions = {}\n): Promise<DetectionResult> {\n  if (!ExpoFaceDetector || !ExpoFaceDetector.detectFaces) {\n    if (global.expo?.modules?.ExponentConstants?.appOwnership === 'expo') {\n      console.warn(\n        [\n          \"ExpoFaceDetector has been removed from Expo Go. To use this functionality, you'll have to create a development build or prebuild using npx expo run:android|ios commands.\",\n          'Learn more: https://expo.fyi/face-detector-removed',\n          'Learn more about development builds: https://docs.expo.dev/develop/development-builds/create-a-build/',\n          'Learn more about prebuild: https://docs.expo.dev/workflow/prebuild/',\n        ].join('\\n\\n')\n      );\n    }\n    throw new UnavailabilityError('expo-face-detector', 'detectFaces');\n  }\n  return await ExpoFaceDetector.detectFaces({ ...options, uri });\n}\n"]}